<!-- 
 【再生スピード】
フェッチと再生が並行して進む構造（fetchTTS と playAudio を分離）。
次の音声再生が事前にフェッチされるため、スムーズな音声再生が可能。
特に複数の短い文を連続で再生する場合、Bのほうが迅速に再生できる。

【音声の重複防止】
音声再生も playNextAudio 関数で管理されますが、再生中に次のフレーズを事前にフェッチする部分が追加されています。
audio.onended イベントの後に次のフレーズが再生される点はAと同じ。
フェッチと再生が並行して行われるため、次のフレーズが速やかに再生されるだけで、再生が重なることはありません。
-->
<!-- static/index.html -->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Azure Speech AI Chat</title>
  <!-- Faviconのリンクを追加 -->
  <link rel="icon" href="/static/favicon.ico" type="image/x-icon" />
  
  <!-- Azure Cognitive Services Speech SDK JS (CDN) -->
  <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
  <style>
    body { font-family: Arial, sans-serif; margin: 50px; }
    #chat { border: 1px solid #ccc; padding: 10px; height: 300px; overflow-y: scroll; }
    .message { margin: 10px 0; }
    .user { color: blue; }
    .ai { color: green; }
    button { padding: 10px 20px; margin: 5px; }
    #status { margin-top: 10px; }
    #result { margin-top: 10px; font-weight: bold; }
    #textInputContainer { margin-top: 20px; }
    #textInput { width: 80%; padding: 10px; }
    #sendBtn { padding: 10px 20px; }
  </style>
</head>
<body>
  <h1>Azure Speech AIチャットアプリ</h1>
  <div id="chat"></div>
  <button id="startBtn">音声認識開始</button>
  <button id="stopBtn" disabled>音声認識停止</button>
  <button id="resetBtn">リセット</button>
  <p id="status"></p>
  <p id="result"></p>
  
  <!-- テキスト入力フィールドと送信ボタンの追加 -->
  <div id="textInputContainer">
    <input type="text" id="textInput" placeholder="メッセージを入力..." />
    <button id="sendBtn">送信</button>
  </div>

  <script>
    let audioConfig, speechRecognizer;
    let accumulatedText = ''; // 認識結果を蓄積する変数
    let conversationHistory = []; // 会話履歴を保持する配列
    let accumulatedPartialText = ''; // AIの部分的な応答を蓄積する変数

    // 音声再生のキューと再生状態の管理
    let audioQueue = [];
    let isPlaying = false;
    let preFetchedAudio = null; // 次に再生する音声データのバッファ

    // #####################################################
    // (A) 新しい関数: "テキストをAIとして表示" + "先にTTSをダウンロード" + "音声再生" を同期的に行う
    // #####################################################
    async function addAiMessageSimultaneous(aiText) {
      try {
        // 1) 先にTTS音声データを取得 (ダウンロード完了待ち)
        const audioBlob = await fetchTTS(aiText);

        // 2) ダウンロード完了後、テキストを画面に表示
        addMessage(aiText, 'ai');

        // 3) すぐに音声再生開始
        await playAudio(audioBlob);

      } catch (err) {
        console.error("addAiMessageSimultaneous error:", err);
        addMessage(aiText + " (音声再生失敗)", 'ai');
      }
    }

    // ローカルストレージから会話履歴を読み込む
    window.onload = async function() {
      // 既存の会話履歴をローカルストレージから読み込み
      const savedHistory = localStorage.getItem('conversationHistory');
      if (savedHistory) {
        conversationHistory = JSON.parse(savedHistory);
        conversationHistory.forEach(msg => addMessage(msg.content, (msg.role === 'user' ? 'user' : 'ai')));
      }

      // ページロード時にウォームアップAPIをコール
      try {
        const warmResp = await fetch("/api/warmup");
        if (warmResp.ok) {
          const warmData = await warmResp.json();
          console.log("Warmup success:", warmData);
          
        } else {
          console.warn("Warmup request failed");
        }
      } catch (err) {
        console.warn("Warmup fetch error:", err);
      }

      // (A) 1文字程度の超短いテキスト (例: "ウォームアップ")
      const dummyShortText = "ウォームアップ";
      try {
        // "/api/tts" にPOSTし、音声を取得。再生せず捨てる。
        console.log("Dummy TTS start for warmup...");
        const resp = await fetch("/api/tts", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: dummyShortText })
        });
        if (resp.ok) {
          // 音声データを受け取るが再生せず破棄
          const audioBlob = await resp.blob();
          console.log("Dummy TTS warmup completed.");
        } else {
          console.warn("Dummy TTS warmup failed:", resp.statusText);
        }
      } catch (err) {
        console.warn("Dummy TTS warmup error:", err);
      }
    };

    // チャット履歴をローカルストレージに保存
    function saveConversation() {
      localStorage.setItem('conversationHistory', JSON.stringify(conversationHistory));
    }

    document.getElementById("startBtn").onclick = async function() {
      document.getElementById("status").innerText = "トークン取得中...";
      accumulatedText = '';        // 音声認識開始前に一時的な蓄積テキストをリセット
      accumulatedPartialText = ''; // AIからの応答テキストもリセット

      try {
        // サーバー(Flask)からSpeechトークンとリージョンをGET
        let resp = await fetch("/token");
        let data = await resp.json();
        if (data.error) {
          throw new Error(data.error);
        }
        let token = data.token;
        let region = data.region;

        // JavaScript SDKのSpeechConfigをToken連携モードで作成
        let speechConfig = SpeechSDK.SpeechConfig.fromAuthorizationToken(token, region);
        speechConfig.speechRecognitionLanguage = "ja-JP"; // 必要に応じて

        // 長い無音に備えてタイムアウトを延長（必要に応じて調整）
        // EndSilenceTimeoutMs を30秒に設定（デフォルトは5000ms）
        speechConfig.setProperty(SpeechSDK.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "30000");

        // 初期無音タイムアウトも延長（必要に応じて）
        speechConfig.setProperty(SpeechSDK.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "10000");

        // ノイズ抑制の有効化（オプション）
        speechConfig.setProperty(SpeechSDK.PropertyId.SpeechServiceConnection_EnableNoiseSuppression, "true");

        // マイクアクセス許可を要求 (ユーザーが「許可」する必要あり)
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();

        // SpeechRecognizerを作成
        speechRecognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);
        document.getElementById("status").innerText = "音声認識を開始します...";

        // イベント: 認識セッション開始
        speechRecognizer.sessionStarted = (s, e) => {
          console.log("sessionStarted");
          document.getElementById("status").innerText = "録音中...(話してください)";
        };

        // イベント: 認識途中のイベント (部分的な結果)
        speechRecognizer.recognizing = (s, e) => {
          console.log("Recognizing:", e.result.text);
          document.getElementById("result").innerText = "途中結果: " + e.result.text;
          // 部分結果は蓄積しない
        };

        // イベント: 認識確定イベント (最終結果)
        speechRecognizer.recognized = (s, e) => {
          console.log("Recognized:", e.result);
          if (e.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
            document.getElementById("result").innerText = "最終結果: " + e.result.text;
            accumulatedText += e.result.text + ' '; // テキストを蓄積
          } else {
            document.getElementById("result").innerText = "認識できませんでした";
          }
        };

        // イベント: セッション終了(マイク停止など)
        speechRecognizer.sessionStopped = (s, e) => {
          console.log("sessionStopped");
          document.getElementById("status").innerText = "認識セッションが終了しました。";
          document.getElementById("stopBtn").disabled = true;
          document.getElementById("startBtn").disabled = false;
          document.getElementById("resetBtn").disabled = false;

          if (accumulatedText.trim() !== '') {
            // ユーザーの発話をチャット欄に追加＆AIにリクエスト
            addMessage(accumulatedText.trim(), 'user');
            // AIにメッセージを送信してストリーミング応答を処理
            sendChatAndStreamResponse(accumulatedText.trim());
          }
          accumulatedText = ''; // 蓄積テキストをリセット
        };

        // イベント: キャンセルやエラー
        speechRecognizer.canceled = (s, e) => {
          console.warn("canceled", e);
          document.getElementById("status").innerText = "キャンセルまたはエラー";
          document.getElementById("stopBtn").disabled = true;
          document.getElementById("startBtn").disabled = false;
          document.getElementById("resetBtn").disabled = false;
          accumulatedText = ''; // 蓄積テキストをリセット
        };

        // 音声認識を開始
        speechRecognizer.startContinuousRecognitionAsync();
        document.getElementById("startBtn").disabled = true;
        document.getElementById("stopBtn").disabled = false;
        document.getElementById("resetBtn").disabled = true; // リセットボタンを無効化
      } catch (err) {
        console.error(err);
        document.getElementById("status").innerText = "エラー: " + err.message;
      }
    };
    // 音声認識停止ボタン
    document.getElementById("stopBtn").onclick = function() {
      if (speechRecognizer) {
        speechRecognizer.stopContinuousRecognitionAsync(
          () => {
            console.log("stop success");
            document.getElementById("status").innerText = "認識停止";
            document.getElementById("stopBtn").disabled = true;
            document.getElementById("startBtn").disabled = false;
            document.getElementById("resetBtn").disabled = false;
          },
          (err) => {
            console.error("stop error", err);
            document.getElementById("status").innerText = "停止中にエラーが発生";
          }
        );
      }
    };

    // リセットボタンの実装
    document.getElementById("resetBtn").onclick = function() {
      conversationHistory = []; // 会話履歴をリセット
      document.getElementById("chat").innerHTML = ''; // チャット表示をクリア
      document.getElementById("result").innerText = ''; // 認識結果表示をクリア
      document.getElementById("status").innerText = '会話がリセットされました。';
      document.getElementById("resetBtn").disabled = true; // リセットボタンを無効化
      localStorage.removeItem('conversationHistory'); // ローカルストレージをクリア
    };
    // チャット欄にメッセージを追加
    function addMessage(message, sender) {
      const chatDiv = document.getElementById('chat');
      const messageDiv = document.createElement('div');
      messageDiv.classList.add('message', sender);
      messageDiv.innerHTML = `<strong>${sender === 'user' ? 'You' : 'AI'}:</strong> ${message}`;
      chatDiv.appendChild(messageDiv);
      chatDiv.scrollTop = chatDiv.scrollHeight;

      // 会話履歴に追加
      if (sender === 'user') {
        conversationHistory.push({ role: 'user', content: message });
      } else if (sender === 'ai') {
        conversationHistory.push({ role: 'assistant', content: message });
        // ★ ここでTTSを呼び出す(最小限変更) ★
        // sendTTSRequest(message);
      }

      // チャット履歴をローカルストレージに保存
      saveConversation();
    }

    async function sendChatAndStreamResponse(userMessage) {
      try {
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({ conversation: conversationHistory }) // 会話履歴を送信
        });

        if (!response.ok) {
          throw new Error('チャットリクエストに失敗しました。');
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder('utf-8');
        let buffer = '';

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n\n');
          buffer = lines.pop(); // 最後の行が未完の可能性があるのでバッファに戻す

          for (const line of lines) {
            if (line.startsWith("data: ")) {
              const jsonData = line.replace("data: ", "");
              try {
                const data = JSON.parse(jsonData);
                if (data.error) {
                  addMessage(data.error, 'ai');
                  continue;
                }
                const content = data.content;
                if (content === "【END】") {
                  // 送信が終了したことを示す
                  if (accumulatedPartialText.trim() !== '') {                    
                    // enqueueTTSRequest(accumulatedPartialText.trim()); // 音声キューに追加
                    // await sendTTSRequest(accumulatedPartialText.trim()); // テキスト表示後に「待って」TTS呼び出ししていた
                    // sendTTSRequest(accumulatedPartialText.trim()); // 非同期で呼び出し (並行実行)
                    // (B) 文末確定 -> Simultaneous表示&音声
                    await addAiMessageSimultaneous(accumulatedPartialText.trim()); 
                    accumulatedPartialText = '';
                  }
                  continue;
                }

                // 従来: while文で文末判定し addMessage, sendTTSRequest していたが削除
                accumulatedPartialText += content;
                let idx;
                while ((idx = accumulatedPartialText.search(/[。！？]/)) !== -1) {
                  const sentence = accumulatedPartialText.slice(0, idx+1);
                  accumulatedPartialText = accumulatedPartialText.slice(idx+1);

                  // ### ここで一文確定 -> 音声DL完了後に画面表示 ###
                  await addAiMessageSimultaneous(sentence);
                }

              } catch (e) {
                console.error("Error parsing JSON:", e);
                continue;
              }
            }
          }
        }

      } catch (err) {
        console.error(err);
        addMessage('サーバーとの通信中にエラーが発生しました。', 'ai');
      }
    }

    // テキスト入力の送信ボタン
    document.getElementById("sendBtn").onclick = function() {
      const textInput = document.getElementById("textInput");
      const message = textInput.value.trim();
      if (message === "") return;
      textInput.value = ""; // 入力フィールドをクリア
      addMessage(message, 'user'); // チャットにユーザーメッセージを追加
      sendChatAndStreamResponse(message); // AIにメッセージを送信
    };

    // Enterキーで送信できるようにする
    document.getElementById("textInput").addEventListener("keypress", function(event) {
      if (event.key === "Enter") {
        event.preventDefault();
        document.getElementById("sendBtn").click();
      }
    });

    // // --------------------------
    // // 音声再生キューの管理ロジック
    // // --------------------------
    // function enqueueTTSRequest(text) {
    //   audioQueue.push(text);
    //   playNextAudio(); // 常に playNextAudio を呼び出す
    // }

    // async function playNextAudio() {
    //   if (isPlaying || audioQueue.length === 0) {
    //     return; // 既に再生中またはキューが空の場合は何もしない
    //   }
    //   isPlaying = true;

    //   const text = audioQueue.shift(); // キューから最初のテキストを取得
    //   let audioBlob;

    //   try {
    //     audioBlob = await fetchTTS(text);
    //   } catch (err) {
    //     console.error("fetchTTS error:", err);
    //     addMessage('音声フェッチ中にエラーが発生しました。', 'ai');
    //     isPlaying = false;
    //     return;
    //   }

    //   try {
    //     // 次の音声を事前にフェッチ開始
    //     if (audioQueue.length > 0) {
    //       preFetchedAudio = fetchTTS(audioQueue[0]);
    //     }

    //     await playAudio(audioBlob).then(() => {
    //       isPlaying = false;
    //       // 再生が終了したら次の音声を再生
    //       playNextAudio();
    //     });
    //   } catch (err) {
    //     console.error("playAudio error:",err);
    //     addMessage('音声再生中にエラーが発生しました。', 'ai');
    //     isPlaying = false;
    //   }
    // }

    // async function sendTTSRequest(text) {
    //   // 音声キューに追加して再生開始
    //   enqueueTTSRequest(text);
    // }
    
    async function fetchTTS(text) {
      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ text: text })
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || 'TTSリクエストに失敗しました。');
      }

      const audioBlob = await response.blob();
      return audioBlob;
    }

    async function playAudio(audioBlob) {
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      return new Promise((resolve, reject) => {
        audio.onended = () => {
          resolve();
        };
        audio.onerror = () => {
          reject(new Error('音声再生に失敗しました。'));
        };
        audio.play();
      });
    }
  </script>
</body>
</html>
